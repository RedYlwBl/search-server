import time
import requests
import json
import logging
from datetime import datetime


# ========== åŸºç¡€é…ç½® ==========
URL = "https://qianfan.baidubce.com/v2/ai_search/web_search"
API_KEY = "bce-v3/ALTAK-BTanKaPjRoEfcyCtkVfDq/329a9a79884358d01ee383eef7254abf373244cb"

HEADERS = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {API_KEY}",
}

# ========== æµ‹è¯•é—®é¢˜é›† ==========
# TEST_QUERIES = [
#     {"type": "æ–°é—»", "query": "ä»Šå¤©æœ‰ä»€ä¹ˆé‡è¦æ–°é—»"},
#     {"type": "å¤©æ°”", "query": "ä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·"},
#     {"type": "æŒ‡æ•°", "query": "ä»Šæ—¥çº³æ–¯è¾¾å…‹æŒ‡æ•°"},
#     {"type": "ç™¾ç§‘", "query": "å…¨å›½æœ‰å¤šå°‘ä¸ªç å¤´"},
# ]

TEST_QUERIES = [
{"type": "", "query": "ä»Šå¤©é’¦å·çš„å¤©æ°”æ€ä¹ˆæ ·å•Šï¼ˆä»Šæ—¥æ—¥æœŸï¼š2025å¹´12æœˆ24æ—¥ï¼‰"},

# {"type": "","query": "æœ€è¿‘é’¦å·ä¼šä¸‹é›¨å—ï¼ˆä»Šæ—¥æ—¥æœŸï¼š2025å¹´12æœˆ24æ—¥ï¼‰"},

# {"type": "","query": "æœ€è¿‘é’¦å·è¿™è¾¹è¿˜ä¼šæœ‰å°é£å—ï¼ˆä»Šæ—¥æ—¥æœŸï¼š2025å¹´12æœˆ24æ—¥ï¼‰"},

# {"type": "","query": "å…¨å›½ä¸€å…±æœ‰å¤šå°‘ä¸ªè‡ªåŠ¨åŒ–ç å¤´ï¼ˆä»Šæ—¥æ—¥æœŸï¼š2025å¹´12æœˆ24æ—¥ï¼‰"},

# {"type": "","query": "å¹¿å·æ¸¯2024å¹´ååé‡"},

# {"type": "","query": "ä¹ è¿‘å¹³æ€»ä¹¦è®°ä»€ä¹ˆæ—¶å€™æ¥çš„åŒ—éƒ¨æ¹¾æ¸¯"},

# {"type": "","query": "ä¹ æ€»ä¹ è¿‘å¹³æ€»ä¹¦è®°æ˜¯ä»€ä¹ˆæ—¶å€™è€ƒå¯Ÿå¹¿è¥¿çš„å‘€"},

# {"type": "","query": "è‚¡å¸‚å¤§ç›˜ï¼ˆä»Šæ—¥æ—¥æœŸï¼š2025å¹´12æœˆ24æ—¥ï¼‰"},
    
]

# ========== æ—¥å¿—é…ç½® ==========
log_filename = f"web_search_test_baidu.log"
logging.basicConfig(
    filename=log_filename,
    filemode="a",
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    encoding="utf-8"
)

def call_web_search(query: str) -> float:
    """è°ƒç”¨æ¥å£å¹¶è¿”å›å»¶è¿Ÿï¼ˆç§’ï¼‰"""
    payload = {
        "messages": [{"role": "user", "content": query}],
        "edition": "lite",
        "search_source": "baidu_search_v2",
        "resource_type_filter": [
            {"type": "web", "top_k": 8},
            {"type": "video", "top_k": 0},
            {"type": "image", "top_k": 0},
            {"type": "aladdin", "top_k": 0},
        ],
    }

    start_time = time.time()
    response = requests.post(
        URL,
        headers=HEADERS,
        data=json.dumps(payload, ensure_ascii=False).encode("utf-8"),
        timeout=60
    )
    end_time = time.time()

    latency = end_time - start_time
    response.encoding = "utf-8"
    refs = json.loads(response.text)["references"]
    
    filtered = [
        {
            "title": r.get("title"),
            "content": r.get("content"),
            "date": r.get("date"),
            "rerank_score": r.get("rerank_score"),
            "authority_score": r.get("authority_score"),
        }
        for r in refs
    ]
    logging.info(
        json.dumps(
            {
                "query": query,
                "latency_sec": round(latency, 3),
                "status_code": response.status_code,
                "response": filtered,
            },
            ensure_ascii=False,
            indent=2   # ğŸ‘ˆ å…³é”®ï¼šæ·»åŠ ç¼©è¿›
        )
    )

    return latency


def main():
    latencies = []

    logging.info("===== å¼€å§‹ Web Search æµ‹è¯• =====")

    for item in TEST_QUERIES:
        qtype = item["type"]
        query = item["query"]

        print(f"æµ‹è¯• [{qtype}]ï¼š{query}")
        latency = call_web_search(query)
        latencies.append(latency)

        print(f"  å»¶è¿Ÿï¼š{latency:.3f} ç§’")
        time.sleep(1.0)

    avg_latency = sum(latencies) / len(latencies)

    logging.info(f"===== æµ‹è¯•å®Œæˆï¼Œå¹³å‡å»¶è¿Ÿï¼š{avg_latency:.3f} ç§’ =====")

    print("\n====== æµ‹è¯•ç»“æœ ======")
    print(f"æµ‹è¯•æ¬¡æ•°ï¼š{len(latencies)}")
    print(f"å¹³å‡å»¶è¿Ÿï¼š{avg_latency:.3f} ç§’")
    print(f"æ—¥å¿—æ–‡ä»¶ï¼š{log_filename}")


if __name__ == "__main__":
    main()
